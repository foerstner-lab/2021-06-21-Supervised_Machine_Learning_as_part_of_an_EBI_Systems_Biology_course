{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "%pip install biopython\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf59cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import numpy as np\n",
    "from Bio import SeqIO, motifs, AlignIO\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from IPython.display import Image\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43fb5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# => Utility functions\n",
    "# Function to read text files of sequences\n",
    "def read_seq_file(in_path):\n",
    "    with open(os.path.abspath(os.path.expanduser(in_path)), \"r\") as f:\n",
    "        sequences = f.read().splitlines()\n",
    "    return sequences\n",
    "\n",
    "# Function to generate list of false sequences\n",
    "def generate_false_sequences(true_seq):\n",
    "    false_seq = []\n",
    "    for seq in true_seq:\n",
    "        x = list(seq)\n",
    "        random.shuffle(x)\n",
    "        false_seq.append(''.join(x))\n",
    "    return false_seq\n",
    "\n",
    "# Function to vectorize each sequence, produces 1d flattened vectors\n",
    "def one_hot_encoder(seq):\n",
    "    integer_encoded = LabelEncoder().fit(np.array([\"A\", \"C\", \"G\", \"T\"])).transform(list(seq)).reshape(len(list(seq)), 1)\n",
    "    return OneHotEncoder(sparse=False, dtype=int, categories=[range(4)]).fit_transform(integer_encoded).flatten()\n",
    "\n",
    "\n",
    "# Function to convert classification prediction into genomic intervals\n",
    "def generate_genomic_locations(classes, proba, window_size):\n",
    "    loc_pairs = []\n",
    "    classes_len = classes.shape[0]\n",
    "    counter = 0\n",
    "    while counter < classes_len: \n",
    "        if classes[counter] == 1:\n",
    "            loc_pairs.append([counter + 1, counter + window_size])\n",
    "            loc_pairs[-1].extend(proba[counter].tolist())\n",
    "        counter += 1\n",
    "    return loc_pairs\n",
    "\n",
    "\n",
    "# Function to convert locations into GFF format\n",
    "def generate_gff_df(locations, seqid, strand, score_filter=0.001):\n",
    "    column_names = [\"seqid\", \"source\", \"type\", \"start\", \"end\", \"score\", \"strand\", \"phase\", \"attribute\"]\n",
    "    strand_letter = \"F\" if strand == \"+\" else \"R\"\n",
    "    # small function to generate attributes\n",
    "    attr_func = lambda row: \\\n",
    "        f\"id={row['seqid']}_{strand_letter}_prom_{row.name}\" \\\n",
    "        f\";name={row['seqid']}_{strand_letter}_prom_{row.name}\" \\\n",
    "        f\";true_proba={row['true_proba']};false_proba={row['false_proba']}\"\n",
    "\n",
    "    gff_df = pd.DataFrame(locations, columns=[\"start\", \"end\", \"true_proba\", \"false_proba\"])\n",
    "    gff_df = gff_df[gff_df[\"true_proba\"] <= score_filter].copy()\n",
    "    gff_df[\"seqid\"] = seqid\n",
    "    gff_df[\"source\"] = \"motif_predictor\"\n",
    "    gff_df[\"type\"] = \"motif\"\n",
    "    gff_df[\"score\"] = \".\"\n",
    "    gff_df[\"strand\"] = strand\n",
    "    gff_df[\"phase\"] = \".\"\n",
    "\n",
    "    for i in gff_df.index:\n",
    "        gff_df.at[i, \"attribute\"] = attr_func(gff_df.loc[i])\n",
    "    gff_df.drop([\"true_proba\", \"false_proba\"], inplace=True, axis=1)\n",
    "    gff_df = gff_df.reindex(columns=column_names)\n",
    "    return gff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155ddb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "promoters_seq_url = \"http://regulondb.ccg.unam.mx/menu/download/datasets/files/PromoterSigma24Set.txt\"\n",
    "genome_file = \"./GCF_000005845.2_ASM584v2_genomic.fa\"\n",
    "true_bs_file = \"./sigE_binding_sites.txt\"\n",
    "save_dir = \"./\"\n",
    "iterations = 500\n",
    "window_size = 29\n",
    "threads = 40\n",
    "score_filter = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90dfb768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0\n",
      "TAGATGTCCTTGATTAACACCAAAATTAAACCTTTTAAAAACCAGGCATTCAAAAACGGC\n",
      ">1\n",
      "AAAGAAAATAATTAATTTTACAGCTGTTAAACCAAACGGTTATAACCTGGTCATACGCAG\n",
      ">2\n",
      "CTGCTGTTCCTTGCGATCGAAAAGATCAAGGGCGGACCGGTATCCGAGCGGGTTCAAGAC\n",
      ">3\n",
      "GCGGAAGCACAAATTGCACCAGGTACGGAACTAAAAGCCGTAGATGGTATCGAAACGCCT\n",
      ">4\n",
      "AAATACTTATGGTGCGCTGGCTTCTTTGGAACTTGCGCAGCAATTTGTTGACAAAAATGA\n",
      ">5\n",
      "ACGCGTTATTAATCAGCGTCTGATGCCATTACACAACAAACTATTTGTCGAACCCAATCC\n",
      ">6\n",
      "CTGATCTAAGCGTTGACCGAGTTGGTTTTCGGACACCGTTGCAGTGAGCTGTACTCGTTG\n",
      ">7\n",
      "TTTTATTCGCGAACGTGAATAATCCGGGAACATTTCGGCCAAAGCCTGATCTAAGCGTTG\n",
      ">8\n",
      "GGCGAATGCGAAAGAACTGCTTGCAGCGTAAACTTTTTTCCTGCTTCACGGTCAGAGTAA\n",
      ">9\n",
      "GCCGTTACACTCAAAGGCGGCGCGGTGGGAACGATATTTCACAGTATCGGTCAAATGACT\n",
      ">10\n",
      "TAAAAAATATCTTGTATGTGATCCAGATCACATCTATCATTTAGTTATCGATCGTTAAGT\n",
      ">11\n",
      "ATTCTGAAAGTTAAAGGGCGCATGAATGAACTTATGGCGCTTCATACGGGTCAATCATTA\n",
      ">12\n",
      "TGGAAAATTCTTAGAAACCGATCACATACAGCTGCATTTATTAAGGTTATCATCCGTTTC\n",
      ">13\n",
      "TTTACCTTTTGCAGAAACTTTAGTTCGGAACTTCAGGCTATAAAACGAATCTGAAGAACA\n",
      ">14\n",
      "ACCGAGCGTCTGCGGCAACTTCATCAACAGCATCACCCGCGGGCGTGATGTCTGAAAAGA\n",
      ">15\n",
      "GGAAAAAAATCTGGCTGAAAATACGTTGAACGCTTACCGTCGCGATCTGTCAATGATGGT\n",
      ">16\n",
      "TTTGTGCTTCATGCACACTCTTTCCCCACACTTTTTCCCTTTGCTGTGGTCTACTTATTC\n",
      ">17\n",
      "TTAATTTTTTCATCGTGAGCCCTTTTTTTGAACTATTATTAAAAAATGATGTCACTGCGC\n",
      ">18\n",
      "GAGACAGCACTCATTTCGCGGTCATCGAAACTAATTTAAACAAAAAGAGTCTGAAAATAG\n",
      ">19\n",
      "ATTTCATCAATGAAATGTAAAAATATATAAACTTGATGATTTAAGCATTTTCTTATACCC\n",
      ">20\n",
      "GATAAATCCATGGCTCTGCGCCTGGCGAACGAACTTTCTGATGCTGCAGAAAACAAAGGT\n",
      ">21\n",
      "CCCTATAAAGTAAACGATGACCCTTCGGGAACTTCAGGGTAAAATGACTATCAAAATGTG\n",
      ">22\n",
      "ATGGAAATAATTCACATTAAAAATAATCGACCCTATGCTTATAAATATAATCAATATATT\n",
      ">23\n",
      "CGCGGCGTTTTTTAATGATCTGGCCCGTGAACTGATTGCTATTATGTTGATCCCTGGGCT\n",
      ">24\n",
      "CGATGACTTTAGGCGATCAATATAAGATCGCCGGGCCACGCAAAGAACTGCACCCTCCGG\n",
      ">25\n",
      "GGCCACGCCAATGATTTTTCCCCAATACTGCATATATTCCCCAAATCGACACACGGATAT\n",
      ">26\n",
      "CAAGTAATATTTGTCATAATGCGCGCTGCAATTTATCCGTATTAAGAGAATCAGATGTCC\n",
      ">27\n",
      "CCTCTGAAGACCTCGTCACGTTATACGGAACAACATTTAACTCCAGCGGTCTGAAAATGC\n",
      ">28\n",
      "ACGCAGCAAGAACGCCGAGCTGATTGAAAAGGTTAGAACATCCTATGAAATTCAAAACAA\n",
      ">29\n",
      "ACGCCGTTGCTTACAACAGCAGCGATGTAAAAGACATCACTGCCGACGTACTGAAACAGG\n",
      ">30\n",
      "TGTCCTGACTAGTCTTTACACTCTTTACAGGAACCATTGTCGTACATGATGGCCCAACCA\n",
      ">31\n",
      "AAAATATCTTTATTTTTGGTCATACCGTGGAACAAGTGAAGGCAATTCTGGCCAAAGGCT\n",
      ">32\n",
      "CTCCGGTAATTTTTTTAAAAATTTTCTGAACTCTTTCTTCCCAGGCGAGTCTGAGTATAT\n",
      ">33\n",
      "GCACTGCAAGGTTTATCAAAAATTATGGAACTTATTGCCCATCGTGATGCTCCAATCATT\n",
      ">34\n",
      "TGGAGTACGCATGGTTTAACAACGTCGAAACCAAACCGGGCATTGGTTATCCGAAAAACT\n",
      ">35\n",
      "TGCTTAATCCATGAGCCAGCGTGCTGAACGATACCGGGATTCTGTTGTCGGAATGGCTGG\n",
      ">36\n",
      "ATCCAGTGGTATTATTGGCCATTGAAAGAACCTTTTTACATTATGAGCGTCAATATCAGT\n",
      ">37\n",
      "GTGAGTTCGGCCTCTGGCACCTGGAGCCGGAACAAATCACTCAGGGCTTTGTCGAATTCC\n",
      ">38\n",
      "GATTCGGATGTGATGGTATGATTACAGACATTCGTGTCTGAGATTGTCTCTGACTCCATA\n",
      ">39\n",
      "CCTGACTACTGTATGTTACTTTATCCTGAACTACGCACCATTGAAGGTGTCTTAAAAAGT\n",
      ">40\n",
      "TACAAATAATGCTGCCACCCTTGAAAAACTGTCGATGTGGGACGATATAGCAGATAAGAA\n",
      ">41\n",
      "TGACAAACAAAAACAGATGCGTTACGGAACTTTACAAAAACGAGACACTCTAACCCTTTG\n",
      ">42\n",
      "TGCGTAATTTATTCACAAGCTTGCATTGAACTTGTGGATAAAATCACGGTCTGATAAAAC\n",
      ">43\n",
      "AAGTGGCGGCGCGTTGAAAGAAATTTCGAACCCTGAGAACTTAATGTTGTCAGAAGAACT\n",
      ">44\n",
      "CAGGAATTAAGGACAGCGGTCATTTAATTCCAGGACACGGTGGTATTTTAGATCGTATTG\n",
      ">45\n",
      "TTATTAATCTGTTAACATTACGTTATCTAAAATATCTGGTAAAAAGTGGACTAAACGGTC\n",
      ">46\n",
      "AAAATGAAATTTTTTTGCACAACCGCAGAACTTTTCCGCAGGGCATCAGTCTTAATTAGT\n",
      ">47\n",
      "TCGAAACAATTCTTATGGTCAGGCTGGCGAACTAAGCGCCTTGCTATGGGTCACAATGGG\n",
      ">48\n",
      "CGTGGGGAAAGTTTTTGGAAAACAACTGCAACTGACCTGCAATAAGAAGGTCAAAGCTAT\n",
      ">49\n",
      "TAAATAAACAATGATGAAATCCAAAATGAAATTGATGCCATTATTGGTGTCAGTAACCTT\n",
      ">50\n",
      "CGCGACCCGTGGGCGGCACAGGTGAGCCAACTTTCGCTACCAAAACTGGTCGAACAGGTG\n",
      ">51\n",
      "ATAAAGTCATTTTTTATTCAAATATAAGGAACTTAATATTTAAAAAATGTTCCATACAAT\n",
      ">52\n",
      "TTTTCAGAAATTGCGCGTAAATTTTTCGCACTTAAAGAATATTTATTAATCTAACGCAAT\n",
      ">53\n",
      "TTTATTTACGTAACAACGTCGCATGGAAGAAACTTCCGGGCAAAGAATGAATCTTAAGAG\n",
      ">54\n",
      "CCGATGATCCTCATCGTAATCCAACCGAAACTTTACCTGATTCTGGCAGTCAAATCGGCT\n",
      ">55\n",
      "GCTTGTTTTTATGAAGTAAAAGAATAACGGCACTTTTTGGTGAATTTGCACTCCAAGCAA\n",
      ">56\n",
      "CAAACTGGATGAGGTACGCACCAGCGAACTTTTCGACGTTTGGTGGGACTAAGAAAGCAT\n",
      ">57\n",
      "AAGCATACGGGCGATGACAAATGCAAAACTGCCTGATGCGCTACGCTTATCAGGCCTGGA\n",
      ">58\n",
      "AATACAGACAAAGAGCATCTGCGAAAAATTGCACGCGGGATGTTCTGGCTGATGCTGCTT\n",
      ">59\n",
      "ATTTTGCGCCAAATTGCCATGCAACGGGCAATTTGACGGGCGTAAAAGTTTGAAGCAGTG\n",
      ">60\n",
      "TTATGTCACATAAGTGACGATGAACGGCGAACTTAATGCGATCTTTTTTGTCAGTAGATA\n",
      ">61\n",
      "TGAGTTCAGGTTTATTTCACTTCTTCCGTGACATTTTCATGTTCTTGCGGTCTAACACGA\n",
      ">62\n",
      "TATATTTGAAATCAAGTTTCGCATATTGAAATTTTAAGCCAAAAAAGCGATCAAAAAAAC\n",
      ">63\n",
      "AAAACAAGTTGATTTGATATATTAAATGAACAAATTAATCTTGATGGCAGTCTGATTATT\n",
      ">64\n",
      "CCAGAACACCCATCAGGCGCGTCCTCATCGGCTACGATGTAAAAATGGGTCTGGAAATGA\n",
      ">65\n",
      "AAAGTTGTAATAAGCTTGTCTGAATCGAACTTTTAGCCGCTTTAGTCTGTCCATCATTCC\n",
      ">66\n",
      "GCAAATGACAATACCCGGAAAATCCTTCTGAACTCTTCACCTTAAGCAATATCAAAAAAA\n",
      ">67\n",
      "GACCTTTGTTACAATTAGATTCAATTTGAATTTATGTTTTTGAATGCTTTCTTATCTCAC\n",
      ">68\n",
      "ATCCGCATTCCTTCGCATCGTAGTGCTCGCATTCAGGAAATGCATATGCTGACGGTAAAT\n",
      ">69\n",
      "AACGGAGCGGCCCAGCTTACCTGCCATTGCACTAAATACTGATAATGTTGTCTTAACGGC\n",
      ">70\n",
      "GCGGGAAATTCGATAAATAGCACATATGATTAAAACTCAGACCCAAGTGGTCGGATCACC\n"
     ]
    }
   ],
   "source": [
    "# Load promoter sequences and analyze them\n",
    "prom_col = [\"id\", \"name\", \"strand\", \"tss\", \"sigma_name\", \"sequence\", \"evidence\", \"confidence\"]\n",
    "promotors_df = pd.read_csv(promoters_seq_url, comment=\"#\", sep=\"\\t\", names=prom_col)\n",
    "promotors_df.dropna(subset = [\"sequence\"], inplace=True)\n",
    "promotors_df = promotors_df[promotors_df[\"confidence\"].isin([\"Strong\", \"Confirmed\"])]\n",
    "for x, y in enumerate(promotors_df[\"sequence\"].tolist()):\n",
    "    print(f\">{x}\\n{y[: 60].upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab61fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet() alignment with 71 rows and 60 columns\n",
      "AAAACAAGTTGATTTGATATATTAAATGAACAAATTAATCTTGA...ATT seq_0\n",
      "AAAATATCTTTATTTTTGGTCATACCGTGGAACAAGTGAAGGCA...GCT seq_1\n",
      "AAAATGAAATTTTTTTGCACAACCGCAGAACTTTTCCGCAGGGC...AGT seq_2\n",
      "AAAGAAAATAATTAATTTTACAGCTGTTAAACCAAACGGTTATA...CAG seq_3\n",
      "AAAGTTGTAATAAGCTTGTCTGAATCGAACTTTTAGCCGCTTTA...TCC seq_4\n",
      "AAATACTTATGGTGCGCTGGCTTCTTTGGAACTTGCGCAGCAAT...TGA seq_5\n",
      "AACGGAGCGGCCCAGCTTACCTGCCATTGCACTAAATACTGATA...GGC seq_6\n",
      "AAGCATACGGGCGATGACAAATGCAAAACTGCCTGATGCGCTAC...GGA seq_7\n",
      "AAGTGGCGGCGCGTTGAAAGAAATTTCGAACCCTGAGAACTTAA...ACT seq_8\n",
      "AATACAGACAAAGAGCATCTGCGAAAAATTGCACGCGGGATGTT...CTT seq_9\n",
      "ACCGAGCGTCTGCGGCAACTTCATCAACAGCATCACCCGCGGGC...AGA seq_10\n",
      "ACGCAGCAAGAACGCCGAGCTGATTGAAAAGGTTAGAACATCCT...CAA seq_11\n",
      "ACGCCGTTGCTTACAACAGCAGCGATGTAAAAGACATCACTGCC...AGG seq_12\n",
      "ACGCGTTATTAATCAGCGTCTGATGCCATTACACAACAAACTAT...TCC seq_13\n",
      "ATAAAGTCATTTTTTATTCAAATATAAGGAACTTAATATTTAAA...AAT seq_14\n",
      "ATCCAGTGGTATTATTGGCCATTGAAAGAACCTTTTTACATTAT...AGT seq_15\n",
      "ATCCGCATTCCTTCGCATCGTAGTGCTCGCATTCAGGAAATGCA...AAT seq_16\n",
      "ATGGAAATAATTCACATTAAAAATAATCGACCCTATGCTTATAA...ATT seq_17\n",
      "...\n",
      "TTTTCAGAAATTGCGCGTAAATTTTTCGCACTTAAAGAATATTT...AAT seq_70\n"
     ]
    }
   ],
   "source": [
    "prom_instances = []\n",
    "for i, prom in enumerate(sorted(promotors_df[\"sequence\"].tolist())):\n",
    "    prom_instances.append(SeqRecord(Seq(prom[: 60].upper()), id=f\"seq_{i}\"))\n",
    "\n",
    "align = MultipleSeqAlignment(prom_instances)\n",
    "print(align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_as_fasta = sequences_as_fasta[: -1]\n",
    "print(promotors_df.shape[0])\n",
    "# Slice each sequence at 60 position as this position is the trascription start then make it upper\n",
    "prom_instances = [Seq(prom[: 60].upper()) for prom in promotors_df[\"sequence\"].tolist()]\n",
    "prom_motifs = motifs.create(prom_instances)\n",
    "print(prom_motifs.consensus)\n",
    "prom_motifs.weblogo(f\"{save_dir}/logo.png\")\n",
    "Image(filename=f\"{save_dir}/logo.png\", width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9c23ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input files\n",
    "\n",
    "\n",
    "save_dir = os.path.abspath(save_dir)\n",
    "genome_file_parsed = SeqIO.parse(os.path.abspath(genome_file), \"fasta\")\n",
    "true_sequences = read_seq_file(true_bs_file)\n",
    "false_sequences = generate_false_sequences(true_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "585feaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize training data sets in a parallel mode and make dataframes of it\n",
    "\n",
    "true_dataset_arr = np.array(list(map(one_hot_encoder, true_sequences)), dtype=np.int32)\n",
    "false_dataset_arr = np.array(list(map(one_hot_encoder, false_sequences)), dtype=np.int32)\n",
    "# Concatenate training datasets\n",
    "full_arr = np.concatenate([true_dataset_arr, false_dataset_arr], axis=0)\n",
    "labels = ([1] * true_dataset_arr.shape[0]) + ([0] * false_dataset_arr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75e18201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing: NC_000913.3\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for prediction\n",
    "f_data_to_predict = {}\n",
    "r_data_to_predict = {}\n",
    "for seq_rec in genome_file_parsed:\n",
    "    print(f\"==> Preparing: {seq_rec.id}\")\n",
    "    # Vectorize the genomic sequence forward and reverse\n",
    "    f_chrom_vector = one_hot_encoder(str(seq_rec.seq))\n",
    "    r_chrom_vector = one_hot_encoder(str(seq_rec.reverse_complement().seq))\n",
    "    # Convert vectorized genome to array of sliding windows\n",
    "    f_data_to_predict[seq_rec.id] = sliding_window_view(f_chrom_vector, window_size * 4)[::4]\n",
    "    r_data_to_predict[seq_rec.id] = sliding_window_view(r_chrom_vector, window_size * 4)[::4]\n",
    "    del f_chrom_vector, r_chrom_vector # free some memory space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "553f22d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training RandomForest model\n",
      "===> Predicting for: NC_000913.3 using RandomForest model\n",
      "Time elapsed for RandomForest model: 0.48 minutes\n",
      "Predicted motifs count: 2535 at threshold 0.1\n",
      "Saving GFF\n",
      "==> Training GradientBoosting model\n",
      "===> Predicting for: NC_000913.3 using GradientBoosting model\n",
      "Time elapsed for GradientBoosting model: 1.45 minutes\n",
      "Predicted motifs count: 19687 at threshold 0.05\n",
      "Saving GFF\n",
      "==> Training AdaBoost model\n",
      "===> Predicting for: NC_000913.3 using AdaBoost model\n",
      "Time elapsed for AdaBoost model: 4.85 minutes\n",
      "Predicted motifs count: 2482 at threshold 0.43\n",
      "Saving GFF\n",
      "==> Training MultiLayerPerceptron model\n",
      "===> Predicting for: NC_000913.3 using MultiLayerPerceptron model\n",
      "Time elapsed for MultiLayerPerceptron model: 0.78 minutes\n",
      "Predicted motifs count: 3165 at threshold 0.005\n",
      "Saving GFF\n"
     ]
    }
   ],
   "source": [
    "models = {\"RandomForest\": RandomForestClassifier(n_jobs=threads),\n",
    "          \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "          \"AdaBoost\": AdaBoostClassifier(),\n",
    "          \"MultiLayerPerceptron\": MLPClassifier(max_iter=iterations)}\n",
    "thresholds = {\"RandomForest\": 0.1,\n",
    "              \"GradientBoosting\": 0.05,\n",
    "              \"AdaBoost\": 0.43,\n",
    "              \"MultiLayerPerceptron\": 0.005}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    t = time.time()\n",
    "    # Train\n",
    "    print(f\"==> Training {model_name} model\")\n",
    "    model.fit(full_arr, labels)\n",
    "    save_df = pd.DataFrame()\n",
    "    for seqid in f_data_to_predict.keys():\n",
    "        print(f\"===> Predicting for: {seqid} using {model_name} model\")        \n",
    "        # Get classification probabilities\n",
    "        f_proba = model.predict_proba(f_data_to_predict[seqid])\n",
    "        r_proba = model.predict_proba(r_data_to_predict[seqid])\n",
    "        # Get classes from propabilities\n",
    "        f_predict_classes = np.array([model.classes_[i] for i in np.argmax(f_proba, axis=1)])\n",
    "        r_predict_classes = np.array([model.classes_[i] for i in np.argmax(r_proba, axis=1)])\n",
    "        # Make genomic coordenates from the classification result\n",
    "        f_locations = generate_genomic_locations(f_predict_classes, f_proba, window_size)\n",
    "         # Filp the array up down for reverse strand\n",
    "        r_locations = generate_genomic_locations(np.flipud(r_predict_classes), np.flipud(r_proba), window_size)\n",
    "        # Convert to GFF format\n",
    "        f_gff = generate_gff_df(f_locations, seq_rec.id, \"+\", thresholds[model_name])\n",
    "        r_gff = generate_gff_df(r_locations, seq_rec.id, \"-\", thresholds[model_name])\n",
    "        save_df = save_df.append(f_gff, ignore_index=True)\n",
    "        save_df = save_df.append(r_gff, ignore_index=True)\n",
    "    \n",
    "    print(f\"Time elapsed for {model_name} model: {round((time.time() - t) / 60, 2)} minutes\")\n",
    "    \n",
    "    print(f\"Predicted motifs count: {save_df.shape[0]} at threshold {thresholds[model_name]}\")\n",
    "    save_df.sort_values(by=[\"seqid\", \"start\", \"end\"], inplace=True)\n",
    "    print(\"Saving GFF\")\n",
    "    save_df.to_csv(os.path.abspath(f\"{save_dir}/predicted_promoters_sk_{model_name}.gff\"),\n",
    "                   sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd42a2c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef9194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ab781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8fcab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba963c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54e2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc4625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
